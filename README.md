# ğŸ¤– Understanding HCQAâ€™s Inference-Guided Reasoning in Egocentric Video QA

This repository provides an in-depth explanation of **HCQA (Hierarchical Comprehension Question Answering)**, the winning method for the **Ego4D EgoSchema 2024** challenge in egocentric video question answering (QA). It also explores enhancements proposed in 2025 that aim to improve video reasoning through query-guided processing, temporal modeling, modular agents, and fine-tuned multimodal LLMs.

---

## ğŸ§  Overview

HCQA addresses long-form egocentric video QA (â‰ˆ3 min videos) through a three-stage pipeline:

1. **Fine-Grained Captioning**
2. **Context Summarization**
3. **Inference-Guided Answering**

This README focuses on the **reasoning component** â€” how it works, its limitations, and improvements proposed in recent research.

---

## ğŸ› ï¸ HCQAâ€™s Reasoning Component: Structure and Process

### ğŸ” Pipeline Overview

1. **Input to the LLM**:  
   The system feeds fine-grained captions (e.g., 4-sec clips), a high-level summary, and the QA pair to a large language model (e.g., GPT-4).

2. **Chain-of-Thought Prompting (CoT)**:  
   The LLM reasons step-by-step before answering, improving performance on complex questions.

3. **In-Context Examples**:  
   HCQA uses few-shot learning with 3 curated QA pairs to guide the model.

4. **Reflection and Self-Check**:  
   The model assigns a confidence score (1â€“10). If the score is below a threshold (e.g., 5), the model is asked to reflect and revise the answer.

### ğŸ’¡ Example

> **Video**: A person washes dishes.  
> **Captions**:  
> - 0:00â€“0:04: â€œPerson scrubs a trayâ€  
> - 0:08â€“0:12: â€œRinses a plateâ€  
> **Summary**: â€œC spends the video washing and rinsing various kitchen utensils.â€  
> **Question**: â€œWhat is the personâ€™s primary activity?â€  
> **Reasoning**: â€œAll actions involve washing kitchen items... so the primary objective is cleaning dishes.â€  
> **Answer**: â€œC is cleaning dishes.â€ (Confidence: 9/10)

---

## âš ï¸ Limitations of HCQAâ€™s Reasoning

| Problem | Description |
|--------|-------------|
| **Caption-Question Misalignment** | Captions are not query-aware, risking missing crucial clues. |
| **Limited Temporal Reasoning** | Summaries donâ€™t capture detailed event sequences or causality. |
| **LLM Hallucinations** | GPT-4 may misinterpret or fabricate details. |
| **Overconfidence** | If wrong but confident, the system won't re-check its answer. |
| **Cost and Latency** | Multiple GPT-4 calls make it slow and expensive for real-time use. |

---

## ğŸš€ Proposed Enhancements

### 1. **Question-Guided Video Processing**
> Captions and summaries should be filtered or generated based on the question itself.  
âœ… *Implementation Idea*: Use a "question-guided captioner" that selects relevant frames and objects before generating descriptions.

---

### 2. **Better Temporal and Causal Modeling**
> Build explicit **timelines** or **event graphs**.  
âœ… *Implementation Idea*: Prompt the model to list events before answering (e.g., â€œUnlock â†’ Enter â†’ Close doorâ€).

---

### 3. **Multi-Step or Modular Reasoning**
> Break down reasoning using multiple specialized agents:
- **Text Agent**: Extracts facts.
- **Visual Agent**: Analyzes frames.
- **Knowledge Agent**: Understands entity relationships.
âœ… *Benefit*: Divide-and-conquer boosts precision and interpretability.

---

### 4. **Uncertainty and Self-Checking Enhancements**
> Use **Self-Consistency** or **Evidence Verification**.
âœ… *Implementation Idea*: Generate multiple reasoning paths and vote; or ask the model to cite evidence for each reasoning step.

---

### 5. **Fine-Tuned Specialized Models**
> Train smaller open-source LLMs (e.g., LLaVA, Qwen-VL) on egocentric QA tasks.  
âœ… *Benefit*: Improves accuracy and speed while reducing prompt complexity.

---

## ğŸ“ˆ Recent Enhancements in 2025

| Approach | Description | Performance |
|---------|-------------|-------------|
| **VideoMultiAgents** | Multi-agent framework with question-guided captioning and scene-graph reasoning. | âœ… +3% accuracy on subset (75.4%) |
| **VideoAgent2** | Uses **uncertainty-aware CoT** and multi-step tool planning to reduce hallucination. | âœ… More reliable answers, fewer errors |
| **Fine-Tuned VideoLLMs** | Fine-tuned models like Video-LLaVA-7B and Qwen-VL-7B on egocentric datasets. | âœ… +13% accuracy over GPT-4 zero-shot |

---

## ğŸ“Š Comparison Table

| Aspect | HCQA (2024) â€“ Inference-Guided Answering | Enhanced Approaches (2025) â€“ Whatâ€™s New |
|--------|------------------------------------------|------------------------------------------|
| **Input to Reasoning** | Captions + summary not query-focused | Use question-guided captions & frame selection |
| **Reasoning Method** | Single LLM, CoT prompting | Modular/multi-step reasoning via agents or planner |
| **Handling Uncertainty** | Reflection based on confidence score | Uncertainty-aware CoT with adaptive data retrieval |
| **Model Training** | Few-shot GPT-4 prompting | Fine-tuned multimodal models for egocentric QA |

---

## âœ… Conclusion

HCQA introduced a powerful paradigm for video QA:  
> â€œDescribe the video â†’ Summarize it â†’ Let the LLM reason aloud to answer.â€

However, its limitations in **temporal reasoning**, **confidence handling**, and **scalability** inspired several enhancements:

- Query-aware input generation
- Structured event modeling
- Multi-agent modular reasoning
- Better self-checking (via uncertainty & voting)
- Domain-specific fine-tuned models

### ğŸ‘¨â€ğŸ« Takeaway for Students

Good video QA =  
**Understanding the video well** + **Thinking about the question like a human would**  
HCQA did the latter well. New models do both. Combining these strategies leads to smarter, more trustworthy answers.

---

## ğŸ“š Sources

- **Haoyu Zhang et al.** â€œHCQA @ Ego4D EgoSchema Challenge 2024.â€ *arXiv preprint* (2024).  
  [arXiv](https://arxiv.org/abs/placeholder)

- **Noriyuki Kugo et al.** â€œVideoMultiAgents: A Multi-Agent Framework for Video Question Answering.â€ *arXiv preprint* (2025).  
  [arXiv](https://arxiv.org/abs/placeholder)

- **Zhuo Zhi et al.** â€œVideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT.â€ *arXiv preprint* (2025).  
  [arXiv](https://arxiv.org/abs/placeholder)

- **Alkesh Patel et al.** â€œAdvancing Egocentric Video Question Answering with Multimodal LLMs.â€ *arXiv preprint* (2025).  
  [arXiv](https://arxiv.org/abs/placeholder)
